

import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
# å¯¼å…¥æˆ‘ä»¬çš„â€œè§£é¢˜å·¥å…·â€
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åŠ è½½å™¨å’Œå‘é‡åº“ä»åœ¨ community ä¸­
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.chat_models import ChatTongyi

from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
# ã€é‡ç‚¹ 3ã€‘æç¤ºè¯æ¨¡æ¿åœ¨è¿™é‡Œ
from langchain_core.prompts import ChatPromptTemplate

# --- ä¿®æ”¹ A éƒ¨åˆ†ï¼šåŠ è½½æ–‡ä»¶ ---
# æŠŠåŸæ¥çš„ python_tutorial.pdf æ¢æˆä½ çš„è´¢æŠ¥æ–‡ä»¶å
# 1. åŠ è½½ç¯å¢ƒ
load_dotenv()



# æ–°å¢ä¸€ä¸ªç‹¬ç«‹çš„ç¿»è¯‘å‡½æ•° (ä¸èµ° RAGï¼Œç›´æ¥é—®å¤§æ¨¡å‹)
# -------------------------------------------------------------
def translate_to_english(text, api_key):
    # ä½¿ç”¨è½»é‡çº§æ¨¡å‹å¿«é€Ÿç¿»è¯‘
    llm_trans = ChatTongyi(model="qwen-turbo", dashscope_api_key=os.getenv('ALIYUN_API_KEY'))
    # ç®€å•çš„ç¿»è¯‘æŒ‡ä»¤
    res = llm_trans.invoke(f"Translate the following Chinese text to English. Only output the translation, do not add any explanation: {text}")
    return res.content




st.set_page_config(page_title="ChatPDF Pro", layout="wide")



# --- 2. é¡µé¢åŸºç¡€è®¾ç½® ---
# --- 3. æ ¸å¿ƒå¤„ç†å‡½æ•° (ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ) ---
# @st.cache_resource ä¿è¯åŒä¸€ä¸ªæ–‡ä»¶ä¸Šä¼ åï¼Œä¸ä¼šå› ä¸ºä½ æ¯æ¬¡æé—®éƒ½é‡æ–°å»åˆ‡åˆ†ã€å»ºåº“
@st.cache_resource
def process_pdf_and_build_rag(uploaded_file):
    """
    æ¥æ”¶ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡ -> ä¿å­˜ä¸´æ—¶æ–‡ä»¶ -> è¯»å– -> åˆ‡åˆ† -> å»ºåº“ -> è¿”å›æ£€ç´¢é“¾
    """
    # A. å¤„ç†ä¸´æ—¶æ–‡ä»¶ (PyPDFLoader éœ€è¦ç‰©ç†è·¯å¾„)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
        try:
            # B. åŠ è½½
            loader = PyPDFLoader(tmp_file_path)
            docs = loader.load()

            # C. åˆ‡åˆ†
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=250
            )
            splits = text_splitter.split_documents(docs)

            # D. å»ºåº“ (è‡ªåŠ¨ä½¿ç”¨ç¯å¢ƒå˜é‡é‡Œçš„ Key)
            embeddings = DashScopeEmbeddings(model="text-embedding-v1",dashscope_api_key=os.getenv('ALIYUN_API_KEY'))
            vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
            retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
            # E. æ„å»ºé“¾
            system_template = """
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æå¸ˆã€‚
            è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ åœ¨æ–‡ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œå°±è¯´ä¸çŸ¥é“ã€‚
            
            <context>
            {context}
            </context>
            """
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_template),
                ("human", "{input}"),
            ])
            llm = ChatTongyi(model="qwen-turbo",dashscope_api_key=os.getenv('ALIYUN_API_KEY')) # è‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡ Key
            
            question_answer_chain = create_stuff_documents_chain(llm, prompt)
            rag_chain = create_retrieval_chain(retriever, question_answer_chain)
            
            return rag_chain

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œä¿æŒæœåŠ¡å™¨å«ç”Ÿ
            os.remove(tmp_file_path)

# --- 4. ç•Œé¢ UI è®¾è®¡ ---

st.title("ğŸ“„ æ™ºèƒ½æ–‡æ¡£åˆ†æåŠ©æ‰‹ (ChatPDF)")
st.caption("åŸºäºé˜¿é‡Œäº‘é€šä¹‰åƒé—® | è‡ªåŠ¨è¯»å–åå°é…ç½®")
st.header("ğŸ“‚ ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ æ–‡æ¡£")
uploaded_file = st.file_uploader("è¯·ä¸Šä¼  PDF æ–‡ä»¶", type=["pdf"])
st.markdown("---")
st.markdown("**è¯´æ˜ï¼š**\n1. ä¸Šä¼ åç³»ç»Ÿä¼šè‡ªåŠ¨è§£æ\n2. è§£æå®Œæˆåå³å¯åœ¨å³ä¾§æé—®")
# ä¸»åŒºåŸŸï¼šèŠå¤©åŒº
if uploaded_file:
    # 1. åªæœ‰ä¸Šä¼ äº†æ–‡ä»¶ï¼Œæ‰å¯åŠ¨ RAG ç³»ç»Ÿ
    try:
        with st.spinner("æ­£åœ¨åˆ†ææ–‡æ¡£ï¼Œå»ºç«‹çŸ¥è¯†åº“... (æ–‡æ¡£è¶Šå¤§è¶Šæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…)"):
            rag_chain = process_pdf_and_build_rag(uploaded_file)
        st.success(f"âœ… æ–‡æ¡£ã€Š{uploaded_file.name}ã€‹è§£æå®Œæˆï¼")
        
        # 2. èŠå¤©ç•Œé¢
        st.markdown("### ğŸ’¬ ç¬¬äºŒæ­¥ï¼šå¼€å§‹æé—®")
        
        # åˆå§‹åŒ–èŠå¤©å†å²
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # æ˜¾ç¤ºå†å²
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        # å¤„ç†è¾“å…¥
        if user_input := st.chat_input("æ¯”å¦‚ï¼šç‰¹æ–¯æ‹‰æœ¬å­£åº¦çš„æ¯›åˆ©ç‡æ˜¯å¤šå°‘ï¼Ÿ"):
            # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)

            # AI å›ç­”
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("ğŸ¤” æ­£åœ¨æ€è€ƒ...")
                
                # è°ƒç”¨ RAG
                response = rag_chain.invoke({"input": user_input})
                answer = response['answer']
                
                message_placeholder.markdown(answer)
                
                # å¯é€‰ï¼šæ˜¾ç¤ºå‚è€ƒæ¥æº (Expander æŠ˜å æ˜¾ç¤º)
                with st.expander("æŸ¥çœ‹ AI å‚è€ƒçš„åŸæ–‡ç‰‡æ®µ"):
                    for i, doc in enumerate(response["context"]):
                        st.markdown(f"**[ç‰‡æ®µ {i+1}] (ç¬¬ {doc.metadata.get('page','?')} é¡µ):**")
                        st.text(doc.page_content[:200] + "...")

            # ä¿å­˜ AI å›ç­”
            st.session_state.messages.append({"role": "assistant", "content": answer})
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œä¿æŒæœåŠ¡å™¨å«ç”Ÿ
        print('å¤±è´¥äº†')

else:
    # å¦‚æœæ²¡ä¸Šä¼ æ–‡ä»¶çš„å¼•å¯¼é¡µ
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ä¸€ä»½ PDF è´¢æŠ¥æˆ–æ–‡æ¡£ã€‚")






