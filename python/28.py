# Day 28: è®°å¿†æŒä¹…åŒ– (Vector DB äº‘ç«¯åŒ–)
# ä»»åŠ¡: ä»æœ¬åœ°å†…å­˜å‘é‡åº“è¿ç§»åˆ°äº‘ç«¯æ•°æ®åº“ã€‚
# æŠ€æœ¯: Pinecone æˆ– Supabase (pgvector)ã€‚
# æ„ä¹‰: å³ä½¿æœåŠ¡å™¨é‡å¯ï¼Œä½ ä¸Šä¼ è¿‡çš„æ–‡æ¡£ç´¢å¼•ä¾ç„¶å­˜åœ¨ã€‚


import os
import shutil
import tempfile
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# --- LangChain ç»„ä»¶ ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.chat_models import ChatTongyi
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- ã€NEWã€‘å¼•å…¥ Pinecone ç»„ä»¶ ---
from langchain_pinecone import PineconeVectorStore

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- åˆå§‹åŒ– APP ---
app = FastAPI(title='Day28 :piencone äº‘ç«¯çŸ¥è¯†åº“')

# å…è®¸è·¨åŸŸ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],    
)

# --- å…¨å±€é…ç½® ---
# ä½ çš„ Pinecone ç´¢å¼•åå­— (å¿…é¡»å’Œä½ åœ¨ç½‘é¡µä¸Šåˆ›å»ºçš„ä¸€æ¨¡ä¸€æ ·ï¼)
# æ¯”å¦‚ä½ åˆšæ‰èµ·åå« day28-ragï¼Œè¿™é‡Œå°±å¡« day28-rag

PINECONE_INDEX_NAME = 'day28-rag'

class ChatRequest(BaseModel):
    query: str
    session_id: str = "default_user"

# --- æ ¸å¿ƒé€»è¾‘ A: ä¸Šä¼ å¹¶å­˜å…¥ Pinecone ---
@app.post("/upload")
async def upload_pdf(
    file: UploadFile = File(...), 
    session_id: str = Form("default_user") 
):
    print(f"ğŸ“¥ æ”¶åˆ°æ–‡ä»¶: {file.filename}, å‡†å¤‡å­˜å…¥ Namespace: {session_id}")
    
    # 1. ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        shutil.copyfileobj(file.file, tmp_file)
        tmp_path = tmp_file.name

    try:
        # 2. åŠ è½½ PDF
        loader = PyPDFLoader(tmp_path)
        docs = loader.load()
        
        # 3. åˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        
        # 4. åˆå§‹åŒ– Embedding æ¨¡å‹
        embeddings = DashScopeEmbeddings(model="text-embedding-v1",dashscope_api_key=os.getenv('ALIYUN_API_KEY'))
        
        print("â˜ï¸ æ­£åœ¨æŠŠæ•°æ®æ¨é€åˆ° Pinecone äº‘ç«¯ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")

        # 5. ã€NEWã€‘å­˜å…¥ Pinecone
        # é‡ç‚¹ï¼šnamespace=session_id å®ç°äº†ç”¨æˆ·æ•°æ®éš”ç¦»ï¼
        PineconeVectorStore.from_documents(
            documents=splits,
            embedding=embeddings,
            index_name=PINECONE_INDEX_NAME,
            namespace=session_id,   # <--- å…³é”®ï¼æ•°æ®è¢«è´´ä¸Šäº†â€œå±äºsession_idâ€çš„æ ‡ç­¾
        )
        print(f"âœ… å­˜å‚¨æˆåŠŸï¼Namespace: {session_id}")
        return {
            "message": "PDF å·²å­˜å…¥äº‘ç«¯æ•°æ®åº“ï¼Œæ°¸ä¹…ä¿å­˜ï¼",
            "filename": file.filename,
            "session_id": session_id
        }
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)



# --- æ ¸å¿ƒé€»è¾‘ B: ä» Pinecone æ£€ç´¢å¹¶å›ç­” (æµå¼) ---
@app.post('/chat/stream')
async def chat_stream(request:ChatRequest):
    print(f"ğŸ” ç”¨æˆ· {request.session_id} æé—®: {request.query}")

    try:
        # 1. å‡†å¤‡ Embedding
        embeddings = DashScopeEmbeddings(model="text-embedding-v1",dashscope_api_key=os.getenv('ALIYUN_API_KEY'))
        # 2. ã€NEWã€‘è¿æ¥åˆ°ç°æœ‰çš„ Pinecone ç´¢å¼•
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªè¿æ¥ï¼Œä¸å†™å…¥ã€‚
        # å¿…é¡»æŒ‡å®š namespaceï¼Œå¦åˆ™æŸ¥ä¸åˆ°åˆšæ‰å­˜çš„æ•°æ®ï¼
        vectorstore = PineconeVectorStore.from_existing_index(
            index_name=PINECONE_INDEX_NAME,
            embedding=embeddings,
            namespace=request.session_id,  # <--- å…³é”®ï¼åªå»è¿™ä¸ªç”¨æˆ·çš„æŠ½å±‰é‡Œæ‰¾

        )
        # 3. è½¬æ¢æˆæ£€ç´¢å™¨
        retriever = vectorstore.as_retriever()

        # 4. å‡†å¤‡å¤§è„‘ (LLM)
        llm = ChatTongyi(model='qwen-turbo',dashscope_api_key=os.getenv("ALIYUN_API_KEY"))

        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäº Context å›ç­”ã€‚
        å¦‚æœ Context é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ä½¿ç”¨ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”ã€‚
        ä½¿ç”¨ Markdown æ ¼å¼ã€‚
        
        <context>
        {context}
        </context>
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])

        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        rag_chain = create_retrieval_chain(retriever, create_stuff_documents_chain(llm, prompt))

        # 6. ç”Ÿæˆå™¨å‡½æ•° (æµå¼è¾“å‡º)
        def generate_response():
            try:
                for chunk in rag_chain.stream({"input": request.query}):
                    if "answer" in chunk:
                        content = chunk["answer"]
                        if content:
                            # ç¨å¾®äººå·¥å»¶è¿Ÿä¸€ç‚¹ç‚¹ï¼Œæå‡ä½“éªŒ
                            yield content
            except Exception as e:
                # å¦‚æœç”Ÿæˆè¿‡ç¨‹ä¸­æ–­ç½‘äº†ï¼Œåœ¨è¿™é‡ŒæŠ¥é”™ç»™å‰ç«¯
                yield f"\n[Error: {str(e)}]"

        # 3. è¿”å›æµ
        return StreamingResponse(
            generate_response(), 
            media_type="text/event-stream"
        )
    except Exception as e:
        # ã€ä¿®å¤ç‚¹ã€‘å¦‚æœè¿ä¸ä¸Š Pineconeï¼Œç›´æ¥æŠ›å‡º HTTP 500 é”™è¯¯ï¼Œè€Œä¸æ˜¯ yield
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))