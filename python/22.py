# Day 22: è®©å¯¹è¯æ›´åƒäºº (Memory & History)
# ä»»åŠ¡: ç»™ AI åŠ ä¸Šâ€œçŸ­æœŸè®°å¿†â€ã€‚
# æŠ€æœ¯: RunnableWithMessageHistoryã€‚
# ç—›ç‚¹è§£å†³: è§£å†³â€œAI æ¯ä¸€å¥è¯éƒ½å¿˜è®°ä¸Šä¸€å¥è¯è¯´äº†å•¥â€çš„é—®é¢˜ï¼Œå®ç°è¿ç»­å¤šè½®å¯¹è¯ã€‚



# ğŸ“ ç°åœ¨çš„æ€»ç»“
# æŠŠä»£ç å¤åŸï¼Œç°åœ¨ä½ å†çœ‹é‚£å‡ è¡Œä»£ç ï¼Œå®ƒä»¬ä¸å†æ˜¯ç¥ç§˜çš„å­—ç¬¦ï¼Œè€Œæ˜¯å…·ä½“çš„å·¥åºï¼š

# PyPDFLoader: æ¬è¿å·¥ï¼ŒæŠŠä¹¦æ¬è¿›ç”µè„‘ã€‚

# RecursiveCharacterTextSplitter: å¨å¸ˆï¼ŒæŠŠç‰›æ’åˆ‡æˆ AI å’¬å¾—åŠ¨çš„å°å—ã€‚

# Chroma & Embeddings: å›¾ä¹¦ç®¡ç†å‘˜ï¼ŒæŠŠæ–‡å­—å˜æˆæ•°å­—ç´¢å¼•ï¼Œæ–¹ä¾¿æŸ¥æ‰¾ã€‚

# history_aware_retriever: ç¿»è¯‘å®˜ï¼ŒæŠŠä½ é‚£å¥å«ç³Šä¸æ¸…çš„â€œå®ƒå¥½å—ï¼Ÿâ€ï¼Œç¿»è¯‘æˆç²¾å‡†çš„æœç´¢æŒ‡ä»¤ã€‚

# ChatTongyi: æœ€ç»ˆçš„è€ƒç”Ÿï¼Œæ ¹æ®æœåˆ°çš„å°æŠ„å†™å‡ºç­”æ¡ˆã€‚


import streamlit as st
import os
import tempfile
from dotenv import load_dotenv

# --- 1. åŸºç¡€é…ç½® ---
load_dotenv()
st.set_page_config(page_title="ChatPDF Pro (è®°å¿†ç‰ˆ)", layout="wide", page_icon="ğŸ§ ")

# æ£€æŸ¥ Key
if not os.getenv("ALIYUN_API_KEY"):
    st.error("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ï¼")
    st.stop()

# --- 2. å¯¼å…¥ç»„ä»¶ ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.chat_models import ChatTongyi
from langchain_text_splitters import RecursiveCharacterTextSplitter

# æ ¸å¿ƒé“¾ç»„ä»¶
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# ã€Day 22 æ ¸å¿ƒç»„ä»¶ã€‘å†å²æ„ŸçŸ¥æ£€ç´¢å™¨
from langchain_classic.chains import create_history_aware_retriever

# --- 3. è¾…åŠ©å‡½æ•°ï¼šç¿»è¯‘æœº ---
def translate_to_english(text):
    """æŠŠä¸­æ–‡é—®é¢˜è½¬æˆè‹±æ–‡å…³é”®è¯ï¼Œæé«˜æ£€ç´¢å‡†ç¡®ç‡"""
    llm_trans = ChatTongyi(model="qwen-turbo",dashscope_api_key=os.getenv('ALIYUN_API_KEY'))
    # ç®€å•çš„ç¿»è¯‘æŒ‡ä»¤
    res = llm_trans.invoke(f"Translate the following Chinese text to English. Only output the translation, do not add any explanation: {text}")
    return res.content

# --- 4. æ ¸å¿ƒå¤„ç†å‡½æ•° (å¸¦è®°å¿†æ„å»º) ---
@st.cache_resource
def process_pdf_and_build_rag(uploaded_file):
    """
    åŠ è½½ PDF -> åˆ‡åˆ† -> å»ºåº“ -> è¿”å›ä¸€ä¸ªã€å¸¦è®°å¿†èƒ½åŠ›ã€‘çš„ RAG é“¾
    """
    # A. ä¸´æ—¶æ–‡ä»¶å¤„ç†
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    try:
        # B. åŠ è½½ & åˆ‡åˆ†
        loader = PyPDFLoader(tmp_file_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
        splits = text_splitter.split_documents(docs)

        # C. å»ºåº“
        embeddings = DashScopeEmbeddings(model="text-embedding-v1",dashscope_api_key=os.getenv('ALIYUN_API_KEY'))
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 6})

        llm = ChatTongyi(model="qwen-turbo",dashscope_api_key=os.getenv('ALIYUN_API_KEY')) # æˆ–è€… qwen-plus

        # --- D. ã€å…³é”®å‡çº§ã€‘æ„å»ºâ€œå†å²æ„ŸçŸ¥â€æ£€ç´¢å™¨ ---
        # å®ƒçš„ä½œç”¨ï¼šå¦‚æœç”¨æˆ·é—®â€œå®ƒå¢é•¿äº†å—ï¼Ÿâ€ï¼Œç»“åˆå†å²æŠŠå®ƒæ”¹æˆâ€œç‰¹æ–¯æ‹‰çš„è¥æ”¶å¢é•¿äº†å—ï¼Ÿâ€
        
        contextualize_q_system_prompt = """
        Given a chat history and the latest user question which might reference context in the chat history, 
        formulate a standalone question which can be understood without the chat history. 
        Do NOT answer the question, just reformulate it if needed and otherwise return it as is.
        """
        
        contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"), # ğŸ‘ˆ è¿™é‡Œçš„å‘å¡«å†å²è®°å½•
            ("human", "{input}"),
        ])
        
        # è¿™ä¸ª retriever ä¼šå…ˆæ€è€ƒæ”¹å†™é—®é¢˜ï¼Œå†å»æœç´¢
        history_aware_retriever = create_history_aware_retriever(
            llm, retriever, contextualize_q_prompt
        )

        # --- E. æ„å»ºå›ç­”é“¾ ---
        
        # è¿™é‡Œçš„ Prompt è´Ÿè´£æ ¹æ®æœåˆ°çš„ç´ æå›ç­”é—®é¢˜
        qa_system_prompt = """
        You are a professional Financial Analyst.
        Use the following pieces of retrieved context to answer the question.
        
        Important Rules:
        1. The Context is likely in English, but you MUST answer in CHINESE (ä¸­æ–‡).
        2. If you don't know the answer, say "è´¢æŠ¥ä¸­æœªæåŠ".
        
        <context>
        {context}
        </context>
        """
        
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"), # ğŸ‘ˆ å›ç­”æ—¶ä¹Ÿè¦çœ‹ä¸€çœ¼å†å²ï¼Œé˜²æ­¢è¯­å¢ƒæ–­è£‚
            ("human", "{input}"),
        ])
        
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        
        # å°†â€œèªæ˜æ£€ç´¢å™¨â€å’Œâ€œå›ç­”é“¾â€ä¸²èµ·æ¥
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        
        return rag_chain

    finally:
        if os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)

# --- 5. ç•Œé¢ UI ---

st.title("ğŸ§  æ™ºèƒ½è´¢æŠ¥åŠ©æ‰‹ (è®°å¿†+ç¿»è¯‘ç‰ˆ)")
st.caption("Day 22 æˆæœï¼šæ”¯æŒå¤šè½®å¯¹è¯ã€ä»£è¯æŒ‡ä»£ã€è·¨è¯­è¨€æ£€ç´¢")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("ğŸ“‚ ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ æ–‡æ¡£")
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼  PDF è´¢æŠ¥", type=["pdf"])

# ä¸»é€»è¾‘
if uploaded_file:
    # 1. å¯åŠ¨
    try:
        with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“..."):
            rag_chain = process_pdf_and_build_rag(uploaded_file)
        st.success("âœ… å¤§è„‘å·²å°±ç»ªï¼")
    except Exception as e:
        st.error(f"å‡ºé”™å•¦: {e}")
        st.stop()

    # 2. åˆå§‹åŒ–å†å²è®°å½• (Session State)
    # ç•Œé¢æ˜¾ç¤ºç”¨
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # LangChain è®°å¿†ä¸“ç”¨ (å­˜å¯¹è±¡)
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # 3. æ˜¾ç¤ºå†å²
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # 4. å¤„ç†è¾“å…¥
    if user_input := st.chat_input("æ¯”å¦‚ï¼šç‰¹æ–¯æ‹‰è¥æ”¶å¤šå°‘ï¼Ÿ(ç„¶åå¯ä»¥æ¥ç€é—®ï¼šé‚£æ¯”å»å¹´é«˜å—ï¼Ÿ)"):
        # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # AI å›ç­”
        with st.chat_message("assistant"):
            with st.status("ğŸ§  AI æ­£åœ¨æ€è€ƒ...", expanded=True) as status:
                
                # Step 1: ç¿»è¯‘ (ä¸ºäº†æœå¾—å‡†)
                status.write("ğŸ”„ 1. æ­£åœ¨ä¼˜åŒ–æœç´¢å…³é”®è¯ (ä¸­->è‹±)...")
                english_query = translate_to_english(user_input)
                status.write(f"ğŸ‡ºğŸ‡¸ æ£€ç´¢è¯: *{english_query}*")
                
                # Step 2: æ£€ç´¢ + ç”Ÿæˆ (å¸¦è®°å¿†)
                status.write("ğŸ” 2. ç»“åˆä¸Šä¸‹æ–‡æ£€ç´¢æ–‡æ¡£...")
                
                # --- å…³é”®è°ƒç”¨ ---
                # åŒæ—¶ä¼ å…¥ input (å½“å‰é—®é¢˜) å’Œ chat_history (è¿‡å»çš„å†å²)
                response = rag_chain.invoke({
                    "input": english_query, 
                    "chat_history": st.session_state.chat_history
                })
                
                status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
            
            answer = response['answer']
            st.markdown(answer)

            # å¼•ç”¨å±•ç¤º
            with st.expander("æŸ¥çœ‹å‚è€ƒåŸæ–‡"):
                for i, doc in enumerate(response["context"]):
                    st.markdown(f"**[ç‰‡æ®µ {i+1}]**")
                    st.caption(doc.page_content[:200] + "...")

        # 5. æ›´æ–°è®°å¿†åº“
        # å­˜å…¥ç•Œé¢å†å²
        st.session_state.messages.append({"role": "assistant", "content": answer})
        
        # å­˜å…¥ LangChain è®°å¿† (æ³¨æ„ï¼šå­˜çš„æ˜¯è‹±æ–‡ query è¿˜æ˜¯ä¸­æ–‡ queryï¼Ÿ)
        # ç­–ç•¥ï¼šä¸ºäº†è®© AI ç†è§£ç”¨æˆ·çš„ä¸­æ–‡è¿½é—®ï¼Œè¿™é‡Œå­˜ã€ä¸­æ–‡åŸè¯ã€‘ä¼šæ›´è‡ªç„¶ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨å†…éƒ¨ç”¨ LLM åšæ”¹å†™
        st.session_state.chat_history.extend([
            HumanMessage(content=user_input), # å­˜ä¸­æ–‡ï¼Œå› ä¸ºæ”¹å†™å™¨èƒ½çœ‹æ‡‚ä¸­æ–‡
            AIMessage(content=answer)
        ])

else:
    st.info("ğŸ‘ˆ è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶")